

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ordinary Least Squares &mdash; pyml-regression-example2 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Stratified sampling" href="stratified_sampling.html" />
    <link rel="prev" title="LinearRegression" href="linear_regression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            pyml-regression-example2
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../theory.html">Theory</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="linear_regression.html">LinearRegression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Ordinary Least Squares</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example">Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relationship-to-the-normal-equations">Relationship to the Normal Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="stratified_sampling.html">Stratified sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="correlation.html">Introduction to Correlation</a></li>
<li class="toctree-l2"><a class="reference internal" href="correlation.html#pearsons-correlation-coefficient-population">Pearson’s Correlation Coefficient (Population)</a></li>
<li class="toctree-l2"><a class="reference internal" href="correlation.html#sample-correlation">Sample Correlation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../results.html">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="../development.html">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../todo.html">TODO</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pyml-regression-example2</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../theory.html">Theory</a></li>
      <li class="breadcrumb-item active">Ordinary Least Squares</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/hakonhagland/pyml-regression-example2/blob/main/docs/theory/least_squares.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ordinary-least-squares">
<h1>Ordinary Least Squares<a class="headerlink" href="#ordinary-least-squares" title="Link to this heading"></a></h1>
<p>The Ordinary Least Squares (OLS) method is a technique for estimating the unknown parameters
in a linear regression model. OLS chooses the parameters of a linear function of a set of
explanatory variables by minimizing the sum of the squares of the differences between the
observed dependent variable in the given dataset and those predicted by the linear function.
In other words, it tries to find the line (or hyperplane) that minimizes the sum of the squared
differences between the observed values and the values predicted by the linear approximation.</p>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Link to this heading"></a></h2>
<p>Assume you have a set of data points <span class="math notranslate nohighlight">\(\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}\)</span>
and you want to find the best line that fits these points in the plane. The equation for the line is of the form:</p>
<div class="math notranslate nohighlight">
\[y = \beta_0 + \beta_1 x\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept, <span class="math notranslate nohighlight">\(\beta_1\)</span> is the slope of the line, <span class="math notranslate nohighlight">\(x\)</span> is the independent variable, and <span class="math notranslate nohighlight">\(y\)</span> is the dependent variable.</p>
<p>The goal of ordinary least squares (OLS) is to find the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimize the sum of the squared differences between the observed values <span class="math notranslate nohighlight">\(y_i\)</span> and the values predicted by the linear approximation <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>.</p>
<p>For each data point <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, the predicted value <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_i = \beta_0 + \beta_1 x_i\]</div>
<p>The residual for each data point is the difference between the observed value and the predicted value:</p>
<div class="math notranslate nohighlight">
\[e_i = y_i - \hat{y}_i\]</div>
<p>The ordinary least squares method minimizes the sum of the squared residuals:</p>
<div class="math notranslate nohighlight" id="equation-eq-ols">
<span class="eqno">(1)<a class="headerlink" href="#equation-eq-ols" title="Link to this equation"></a></span>\[S(\beta_0, \beta_1) = \min_{\beta_0, \beta_1} \sum_{i=1}^{n} e_i^2 = \min_{\beta_0, \beta_1} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2\]</div>
<p>To minimize this sum, we take the partial derivatives of <span class="math notranslate nohighlight">\(S\)</span> with respect to <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> and set them to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial S}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial S}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i (y_i - \beta_0 - \beta_1 x_i) = 0\]</div>
<p>Solving these equations gives the values of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> that minimize the sum of the squared residuals.</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-1">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-beta-1" title="Link to this equation"></a></span>\[\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\bar{y}\)</span> are the means of the independent and dependent variables,
respectively. Once we have <span class="math notranslate nohighlight">\(\beta_1\)</span>, we can find <span class="math notranslate nohighlight">\(\beta_0\)</span> using the formula:</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-0">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-beta-0" title="Link to this equation"></a></span>\[\beta_0 = \bar{y} - \beta_1 \bar{x}\]</div>
<p>Intuitively, we can see <span class="math notranslate nohighlight">\(\beta_1\)</span> as the slope of the line, and <span class="math notranslate nohighlight">\(\beta_0\)</span> as the intercept.</p>
<p>You can imagine a cloud of data points scattered on a 2D plot.
The OLS method fits a straight line through these points such that the vertical distances
(errors) between the points and the line are as small as possible on average, and the
squared differences are minimized.</p>
</section>
<section id="relationship-to-the-normal-equations">
<h2>Relationship to the Normal Equations<a class="headerlink" href="#relationship-to-the-normal-equations" title="Link to this heading"></a></h2>
<p>Equations <a class="reference internal" href="#equation-eq-beta-1">(2)</a> and <a class="reference internal" href="#equation-eq-beta-0">(3)</a> can be reformulated as a linear regression problem
leading to the so-called normal equations.
In linear regression, we aim to find a vector of coefficients <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that
best fits the data. Given:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: the design matrix of shape <span class="math notranslate nohighlight">\((n, p)\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is the number of samples and <span class="math notranslate nohighlight">\(p\)</span> is the number
of features</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\)</span>: the target vector of shape <span class="math notranslate nohighlight">\((n,)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}\)</span>: the vector of coefficients of shape <span class="math notranslate nohighlight">\((p,)\)</span></p></li>
</ul>
<p>Our goal is to find <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that minimizes the residual sum of squares (RSS):</p>
<div class="math notranslate nohighlight">
\[\min_w J(w) = \min_w \|Xw - y\|_2^2\]</div>
<p>Compare this equation with <a class="reference internal" href="#equation-eq-ols">(1)</a>. The two equations are equivalent with
<span class="math notranslate nohighlight">\(\mathbf{X} = [1, x_1; 1, x_2; \dots; 1, x_n]\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y} = [y_1, y_2, \dots, y_n]\)</span>.
And the solution to the normal equations is the same as the solution to the OLS problem.</p>
<p>The objective function (cost function) <span class="math notranslate nohighlight">\(J(w)\)</span> represents the sum of squared differences
between the observed targets and the predicted values:</p>
<div class="math notranslate nohighlight">
\[J(w) = (Xw - y)^T (Xw - y)\]</div>
<p>The expands to:</p>
<div class="math notranslate nohighlight">
\[J(w) = w^T X^T X w - 2 w^T X^T y + y^T y\]</div>
<p>Note that <span class="math notranslate nohighlight">\(y^T y\)</span> is a constant with respect to <span class="math notranslate nohighlight">\(w\)</span> and can be ignored during optimization.</p>
<p>To find the minimum of <span class="math notranslate nohighlight">\(J(w)\)</span>, we compute its gradient with respect to <span class="math notranslate nohighlight">\(w\)</span> and set it to zero:</p>
<div class="math notranslate nohighlight">
\[\nabla_w J(w) = 2 X^T X w - 2 X^T y\]</div>
<p>Setting the gradient to zero to find the critical points:</p>
<div class="math notranslate nohighlight">
\[2 X^T X w - 2 X^T y = 0\]</div>
<p>Simplify by dividing both sides by 2:</p>
<div class="math notranslate nohighlight">
\[X^T X w = X^T y\]</div>
<p>This equation is known as the normal equation.</p>
<p>Assuming <span class="math notranslate nohighlight">\(X^T X\)</span> is invertible (which requires that <span class="math notranslate nohighlight">\(X\)</span> has full rank),
we can solve for <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[w = (X^T X)^{-1} X^T y\]</div>
<p>This is the closed-form solution to the linear regression problem.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="linear_regression.html" class="btn btn-neutral float-left" title="LinearRegression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="stratified_sampling.html" class="btn btn-neutral float-right" title="Stratified sampling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Håkon Hægland.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>